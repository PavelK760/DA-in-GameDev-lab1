# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил:
- Карабанов Павел Александрович
- РИ210942
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 80 |
| Задание 2 | * | 20 |


знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:


[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Научиться интегрировать экономическую систему в проект Unity и обучать ML-Agent.

## Задание 1.
### Измените параметры файла. yaml-агента и определить какие параметры и как влияют на обучение модели.
#### Открыл проект и подключил библиотеки мл агента, запустил обучение агента
![Снимок1](https://user-images.githubusercontent.com/104727697/205080639-bb3fdd4a-2f0c-498e-ae46-65532541e337.png)

#### Агент закончил обучение. Файлы сохранены
![Снимок2](https://user-images.githubusercontent.com/104727697/205099646-ce91fb04-a62f-4b0c-b398-75ae33ec0284.PNG)
![image](https://user-images.githubusercontent.com/104727697/205100006-dc86d1d3-89e2-4eac-8d38-c69d598d40d4.png)

#### Далее установил в виртуальную среду TensorFlow и открыл TensorBoard. Получились следующие графики
![Снимок3](https://user-images.githubusercontent.com/104727697/205100097-d57d0ba1-8a6b-460a-8359-64f7f5bded47.PNG)

#### Теперь начинаем менять пункты по одному и анализировать изменение графиков 
Изначальное содержимое файла yaml
```C#
behaviors:  /* Основной раздел файла конфигурации тренера — это набор конфигураций для каждого поведения в сцене. */
  Economic:  /* Название нашей конфигурации. */
    trainer_type: ppo   /* Сам параметр отвечает за тип используемого тренера (по умолчанию ppo). */
    hyperparameters: /* Это раздел настройки гиперпараметров для агента. */
      batch_size: 1024  /* Количество опыта на каждой итерации градиентного спуска. */
      buffer_size: 10240    /*Количество опытов, которые необходимо собрать перед обновлением модели политики. */
      learning_rate: 3.0e-4 /* Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска. */
      learning_rate_schedule: linear    /* Параметр изменения скорости обучения с течением времени. { linear } скорость обучения уменьшается линейно, достигая 0 на max_steps */
      beta: 1.0e-2  /* beta - сила регуляризации энтропии, которая делает политику «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. */
      epsilon: 0.2  /* Параметр допустимого расхождения между старой и новой политикой. */
      lambd: 0.95    /* Параметр регуляризации (лямбда), насколько агент полагается на свою текущую оценку стоимости при расчете обновленной оценки стоимости. */
      num_epoch: 3       /* Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Кол-во эпох, проще говоря */
      
    network_settings: /* Раздел, содержащий настройки нейронной сети. */
      normalize: false  /*  Параметр определяет применяется ли нормализация к входным данным векторных наблюдений. */
      hidden_units: 128  /* Число нейронов в скрытых слоях. */
      num_layers: 2 /* Число скрытых слоёв сети */
      
    reward_signals: /* Раздел позволяет задавать настройки как для внешних и внутренних сигналов вознаграждения. */
      extrinsic: /* Dнешние сигналы вознаграждения */
        gamma: 0.99 /* Фактор скидки для будущих вознаграждений, поступающих из окружающей среды. Это можно рассматривать как то, как далеко в будущем агент #должен заботиться о возможных вознаграждениях. */
        strength: 1.0   /* коэффициент на который умножается вознаграждение.  */
    checkpoint_interval: 500000 
    max_steps: 750000   /* Задаёт максимальное количество повторов симуляции сцены. */
    time_horizon: 64    /* Количество циклов ML агента, хранящихся в буфере до ввода в модель. */
    summary_freq: 5000  /* Здесь задаётся частота сохранения статистики тренировок по шагам. */
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
```
- Нам необходимо отслеживать `Cumulative Reward` в первую очередь, его и будем анализировать. Фоновую заливку сайта изменил, потому что белая на гитхабе очень плохо смотрится


1. Изменил `learning_rate.` Попробовал увеличить его до `3.0e-3.` Обновлённые графики: 
![image](https://user-images.githubusercontent.com/104727697/205107634-3b48b1b6-915d-4aca-9f50-0ec982d93ea0.png)


1.1 `Cumulative Reward` стал вести себя нестабильно. Вознаграждение стало возрастать гораздо медленнее, а потом вовсе начало уменьшаться. Уменьшил значение до `learning_rate: 3.0e-5. `
![image](https://user-images.githubusercontent.com/104727697/205106106-ceb70946-07be-4837-b474-7ac2e5d0269f.png)
График стал чуть лучше, вознаграждение возрастает, хоть и не монотонно и довольно медленно: 

2 Изменил параметр `epsilon` Попробовал увеличить его до 0.6, графики приобрели следующий вид: 
![image](https://user-images.githubusercontent.com/104727697/205111008-c116781c-5259-44d6-8875-84e91121d121.png)
`Cumulative Reward` представляет из себя константу, равную максимально возможному значению.
2.1 При уменьшении значени ниже 0.35 график возвращается в прежнее состояние:
![image](https://user-images.githubusercontent.com/104727697/205111871-340f39c1-5a62-4830-ad04-c33c3aed641e.png)

3 Сократил значение параметра `num_epoch` в 3 раза. График ведёт себя практически без изменений:
![image](https://user-images.githubusercontent.com/104727697/205112496-d6577867-d785-4f83-acdf-42bd6c41c989.png)
3.1 Однако при увеличении параметра до значения 9, ситуация изменилась, теперь график идёт гораздо стабильнее, удерживая чёткий темп обучения, но зато упала скорость обучния:
![image](https://user-images.githubusercontent.com/104727697/205113061-d7810fb1-f3d3-4875-adde-38c14a13d43e.png)

4. После всего вышеописанного поменял параметр `lambd` с 0.95 на 0.78. Обновлённые графики:
![image](https://user-images.githubusercontent.com/104727697/205109525-30b5ccd8-6319-41bf-a571-523dec8e82f0.png)
график повёл себя довольно странно, с одной стороны он стал очень стабилен, но скорость обучения пула практически до нуля. П 



## Задание 2
### Опишите результаты, выведенные в TensorBoard.
#### Environment
  1. Cumulative Reward - среднее общее вознаграждение за эпизод для всех агентов. Увеличивается, когда эпизод обучения успешен. График должен постоянно увеличиваться, но может вести себя скачкообразно в зависимости от выставленных значений. Обычно стоит максимизировать значения, повышающиие результаты этого параметра.

  2. Episode Length - средняя продолжительность эпизода обучения в среде для агентов.

#### Losses
  1. Policy Loss - - этот график определяет величину изменения политики со временем. Политика — это элемент, определяющий действия, и в общем случае этот график должен стремиться вниз, показывая, что политика всё лучше принимает решения.

  2. Value Loss - средняя потеря функции значения. Она моделирует, насколько хорошо агент прогнозирует значение своего следующего состояния. Должна увеличиваться, пока агент обучается, а затем уменьшаться, когда вознаграждение стабилизируется.

#### Policy
  1. Entropy - график случайности решений модели. Должен уменьшаться во время успешного эпизода.
  1.1 Beta - гиперпараметр для настройки Entropy.
  1.2 Epsilon - гиперпараметр, влияет на скорость развития политики.
  2. Learning Rate - показывает величину шага при поиске оптимальной политики. Должен уменьшаться линейно.
  3. Extrinsic Reward - соответствует среднему совокупному вознаграждению, полученному от окружающей среды за эпизод.
  4. Value Estimate - это среднее значение, посещённое всеми состояниями агента. Чтобы отражать увеличение знаний агента, это значение должно расти, а затем стабилизироваться.


#### Self play
  1. ELO - показывает силу сети.

## Выводы

Я ознакомился с основными операторами зыка Python на примере реализации линейной регрессии, а так же установил на свой компьютер необходимые для дальнейшего обучения ресурсы и получил базовые навыки в обращении с ними
В ходе лабораторной работы, я узнал как можно интегрировать экономическую, систему созданную на основе ML-Agent в проект Unity . Обучил ML-Agent и узнал как параметры конфигурации нейросети влияют на обучение модели. Научился выводить графики в TensorBoard и анализировать их.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
